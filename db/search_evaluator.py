#!/usr/bin/env python3
"""
BM25å…³é”®è¯æœç´¢å¼•æ“
æ”¯æŒä¸­æ–‡åˆ†è¯å’Œå…³é”®è¯æ£€ç´¢
"""

import json
import jieba
import pickle
import os
import re
from typing import List, Dict, Any, Tuple, Optional
from rank_bm25 import BM25Okapi, BM25L, BM25Plus
import numpy as np
import logging
from datetime import datetime
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ChineseTokenizer:
    """ä¸­æ–‡åˆ†è¯å™¨"""
    
    def __init__(self):
        self.stop_words = self._load_stop_words()
        # é…ç½®jiebaç”¨æˆ·è¯å…¸ï¼ˆå¯é€‰ï¼‰
        self._setup_jieba()
    
    def _load_stop_words(self) -> set:
        """åŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨"""
        # åŸºç¡€åœç”¨è¯
        basic_stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 
            'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 
            'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä¸ª', 'æ¥', 'ç”¨',
            'å¤§', 'é‡Œ', 'ä¸º', 'å­', 'ä¸­', 'ä¸', 'ä»', 'åœ°', 'ä»–', 'æ—¶',
            'éƒ¨', 'å°†', 'æˆ', 'è¢«', 'è€Œ', 'æ‰€', 'ä»¥', 'åŠ', 'å…¶', 'å¯¹',
            'å¯ä»¥', 'å¯èƒ½', 'åº”è¯¥', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å',
            'è¿˜æœ‰', 'æˆ–è€…', 'ä»¥åŠ', 'æ ¹æ®', 'é€šè¿‡', 'ç”±äº', 'å…³äº'
        }
        return basic_stop_words
    
    def _setup_jieba(self):
        """é…ç½®jiebaåˆ†è¯å™¨"""
        # æ·»åŠ è‡ªå®šä¹‰è¯å…¸ï¼ˆé’ˆå¯¹ç”µå•†é¢†åŸŸï¼‰
        custom_words = [
            'é—²é±¼', 'äºŒæ‰‹', 'å…¨æ–°', '9æˆæ–°', '8æˆæ–°', '7æˆæ–°', '99æ–°', '95æ–°',
            'iPhone', 'iPad', 'MacBook', 'AirPods', 'iPhone15', 'iPhone14',
            'åä¸º', 'å°ç±³', 'é­…æ—', 'OPPO', 'vivo', 'ä¸€åŠ ', 'è£è€€',
            'åŒ…é‚®', 'æ€¥è½¬', 'å­¦ç”Ÿå…š', 'æ¬å®¶', 'é—²ç½®', 'ä½ä»·', 'æ­£å“',
            'é¢äº¤', 'å½“é¢äº¤æ˜“', 'æ”¯æŒé€€æ¢', 'è´¨é‡ä¿è¯'
        ]
        
        for word in custom_words:
            jieba.add_word(word)
        
        # å¼ºåˆ¶è½½å…¥jiebaè¯å…¸
        jieba.initialize()
        logger.info("jiebaåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def tokenize(self, text: str) -> List[str]:
        """
        åˆ†è¯å¹¶æ¸…ç†
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not text or not isinstance(text, str):
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = self._clean_text(text)
        
        # ä½¿ç”¨jiebaåˆ†è¯
        tokens = jieba.lcut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_tokens = []
        for token in tokens:
            token = token.strip()
            if (len(token) >= 1 and 
                token not in self.stop_words and 
                not self._is_punctuation(token) and
                not self._is_number_only(token)):
                filtered_tokens.append(token.lower())
        
        return filtered_tokens
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fff\-]', ' ', text)
        return text.strip()
    
    def _is_punctuation(self, token: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡ç‚¹ç¬¦å·"""
        return re.match(r'^[^\w\u4e00-\u9fff]+$', token) is not None
    
    def _is_number_only(self, token: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºçº¯æ•°å­—"""
        return token.isdigit()

class BM25SearchEngine:
    """BM25å…³é”®è¯æœç´¢å¼•æ“"""
    
    def __init__(self, 
                 bm25_type: str = "BM25Okapi",
                 k1: float = 1.5, 
                 b: float = 0.75):
        """
        åˆå§‹åŒ–BM25æœç´¢å¼•æ“
        
        Args:
            bm25_type: BM25ç®—æ³•ç±»å‹ ("BM25Okapi", "BM25L", "BM25Plus")
            k1: BM25å‚æ•°k1
            b: BM25å‚æ•°b
        """
        self.tokenizer = ChineseTokenizer()
        self.bm25_type = bm25_type
        self.k1 = k1
        self.b = b
        
        # ç´¢å¼•æ•°æ®
        self.product_bm25 = None
        self.qa_bm25 = None
        self.product_docs = []
        self.qa_docs = []
        self.product_metadata = []
        self.qa_metadata = []
        
        logger.info(f"BM25æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œç®—æ³•ç±»å‹: {bm25_type}")
    
    def _get_bm25_class(self):
        """è·å–BM25ç®—æ³•ç±»"""
        if self.bm25_type == "BM25L":
            return BM25L
        elif self.bm25_type == "BM25Plus":
            return BM25Plus
        else:
            return BM25Okapi
    
    def load_data(self, data_file: str = "embeddings_data/processed_products.json",
                  qa_file: str = "embeddings_data/processed_qa_pairs.json"):
        """
        åŠ è½½æ•°æ®
        
        Args:
            data_file: å•†å“æ•°æ®æ–‡ä»¶è·¯å¾„
            qa_file: é—®ç­”æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        logger.info("å¼€å§‹åŠ è½½æ•°æ®...")
        
        # åŠ è½½å•†å“æ•°æ®
        with open(data_file, 'r', encoding='utf-8') as f:
            products = json.load(f)
        
        # åŠ è½½é—®ç­”æ•°æ®
        with open(qa_file, 'r', encoding='utf-8') as f:
            qa_pairs = json.load(f)
        
        logger.info(f"åŠ è½½å®Œæˆ: {len(products)}ä¸ªå•†å“, {len(qa_pairs)}ä¸ªé—®ç­”å¯¹")
        
        # æ„å»ºç´¢å¼•
        self._build_product_index(products)
        self._build_qa_index(qa_pairs)
    
    def _build_product_index(self, products: List[Dict]):
        """æ„å»ºå•†å“BM25ç´¢å¼•"""
        logger.info("æ„å»ºå•†å“BM25ç´¢å¼•...")
        
        self.product_docs = []
        self.product_metadata = []
        
        for product in products:
            # ç»„åˆæ–‡æœ¬ç”¨äºæ£€ç´¢
            searchable_text = self._prepare_product_search_text(product)
            
            # åˆ†è¯
            tokens = self.tokenizer.tokenize(searchable_text)
            self.product_docs.append(tokens)
            
            # ä¿å­˜å…ƒæ•°æ®
            self.product_metadata.append(product)
        
        # åˆ›å»ºBM25ç´¢å¼•
        bm25_class = self._get_bm25_class()
        if self.bm25_type == "BM25Okapi":
            self.product_bm25 = bm25_class(self.product_docs, k1=self.k1, b=self.b)
        else:
            self.product_bm25 = bm25_class(self.product_docs)
        
        logger.info(f"å•†å“BM25ç´¢å¼•æ„å»ºå®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(self.product_docs)}")
    
    def _build_qa_index(self, qa_pairs: List[Dict]):
        """æ„å»ºé—®ç­”BM25ç´¢å¼•"""
        logger.info("æ„å»ºé—®ç­”BM25ç´¢å¼•...")
        
        self.qa_docs = []
        self.qa_metadata = []
        
        for qa in qa_pairs:
            # ç»„åˆé—®é¢˜å’Œç­”æ¡ˆç”¨äºæ£€ç´¢
            searchable_text = f"{qa.get('question', '')} {qa.get('answer', '')}"
            
            # åˆ†è¯
            tokens = self.tokenizer.tokenize(searchable_text)
            self.qa_docs.append(tokens)
            
            # ä¿å­˜å…ƒæ•°æ®
            self.qa_metadata.append(qa)
        
        # åˆ›å»ºBM25ç´¢å¼•
        bm25_class = self._get_bm25_class()
        if self.bm25_type == "BM25Okapi":
            self.qa_bm25 = bm25_class(self.qa_docs, k1=self.k1, b=self.b)
        else:
            self.qa_bm25 = bm25_class(self.qa_docs)
        
        logger.info(f"é—®ç­”BM25ç´¢å¼•æ„å»ºå®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(self.qa_docs)}")
    
    def _prepare_product_search_text(self, product: Dict) -> str:
        """å‡†å¤‡å•†å“æœç´¢æ–‡æœ¬"""
        components = []
        
        # æ ‡é¢˜ï¼ˆæœ€é‡è¦ï¼Œé‡å¤ä»¥å¢åŠ æƒé‡ï¼‰
        if product.get('title'):
            title = product['title']
            components.extend([title, title])
        
        # å“ç‰Œå’Œå‹å·
        if product.get('brand'):
            components.append(product['brand'])
        if product.get('model'):
            components.append(product['model'])
        
        # åˆ†ç±»ä¿¡æ¯
        if product.get('category'):
            components.append(product['category'])
        if product.get('subcategory'):
            components.append(product['subcategory'])
        
        # æè¿°ï¼ˆæˆªå–é‡è¦éƒ¨åˆ†ï¼‰
        if product.get('description'):
            desc = product['description'][:100]  # é™åˆ¶é•¿åº¦
            components.append(desc)
        
        # æˆè‰²å’ŒçŠ¶æ€
        if product.get('condition'):
            components.append(product['condition'])
        
        return ' '.join(components)
    
    def search_products(self, 
                       query: str, 
                       top_k: int = 10,
                       filters: Optional[Dict] = None) -> List[Dict]:
        """
        æœç´¢å•†å“
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.product_bm25:
            raise ValueError("å•†å“ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨load_data()")
        
        # æŸ¥è¯¢åˆ†è¯
        query_tokens = self.tokenizer.tokenize(query)
        if not query_tokens:
            return []
        
        # BM25æœç´¢
        scores = self.product_bm25.get_scores(query_tokens)
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(scores)[::-1]
        
        results = []
        for idx in top_indices:
            if len(results) >= top_k:
                break
            
            score = scores[idx]
            if score <= 0:  # è·³è¿‡æ— å…³ç»“æœ
                continue
            
            product = self.product_metadata[idx].copy()
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if filters and not self._apply_product_filters(product, filters):
                continue
            
            product['score'] = float(score)
            product['search_type'] = 'bm25'
            results.append(product)
        
        return results
    
    def search_qa_pairs(self, 
                       query: str, 
                       top_k: int = 10,
                       filters: Optional[Dict] = None) -> List[Dict]:
        """
        æœç´¢é—®ç­”å¯¹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.qa_bm25:
            raise ValueError("é—®ç­”ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨load_data()")
        
        # æŸ¥è¯¢åˆ†è¯
        query_tokens = self.tokenizer.tokenize(query)
        if not query_tokens:
            return []
        
        # BM25æœç´¢
        scores = self.qa_bm25.get_scores(query_tokens)
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(scores)[::-1]
        
        results = []
        for idx in top_indices:
            if len(results) >= top_k:
                break
            
            score = scores[idx]
            if score <= 0:  # è·³è¿‡æ— å…³ç»“æœ
                continue
            
            qa = self.qa_metadata[idx].copy()
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if filters and not self._apply_qa_filters(qa, filters):
                continue
            
            qa['score'] = float(score)
            qa['search_type'] = 'bm25'
            results.append(qa)
        
        return results
    
    def _apply_product_filters(self, product: Dict, filters: Dict) -> bool:
        """åº”ç”¨å•†å“è¿‡æ»¤æ¡ä»¶"""
        # åˆ†ç±»è¿‡æ»¤
        if 'category' in filters:
            if product.get('category') != filters['category']:
                return False
        
        if 'subcategory' in filters:
            if product.get('subcategory') != filters['subcategory']:
                return False
        
        # å“ç‰Œè¿‡æ»¤
        if 'brand' in filters:
            if product.get('brand') != filters['brand']:
                return False
        
        # ä»·æ ¼èŒƒå›´è¿‡æ»¤
        if 'price_range' in filters:
            min_price, max_price = filters['price_range']
            price = product.get('price', 0)
            if not (min_price <= price <= max_price):
                return False
        
        # åœ°åŒºè¿‡æ»¤
        if 'location' in filters:
            if product.get('location') != filters['location']:
                return False
        
        # çŠ¶æ€è¿‡æ»¤
        if 'status' in filters:
            if product.get('status') != filters['status']:
                return False
        
        return True
    
    def _apply_qa_filters(self, qa: Dict, filters: Dict) -> bool:
        """åº”ç”¨é—®ç­”è¿‡æ»¤æ¡ä»¶"""
        # é—®é¢˜ç±»å‹è¿‡æ»¤
        if 'question_type' in filters:
            if qa.get('question_type') != filters['question_type']:
                return False
        
        return True
    
    def save_index(self, save_dir: str = "bm25_index"):
        """ä¿å­˜BM25ç´¢å¼•"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜å•†å“ç´¢å¼•
        if self.product_bm25:
            with open(os.path.join(save_dir, "product_bm25.pkl"), 'wb') as f:
                pickle.dump(self.product_bm25, f)
            
            with open(os.path.join(save_dir, "product_docs.pkl"), 'wb') as f:
                pickle.dump(self.product_docs, f)
            
            with open(os.path.join(save_dir, "product_metadata.json"), 'w', encoding='utf-8') as f:
                json.dump(self.product_metadata, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é—®ç­”ç´¢å¼•
        if self.qa_bm25:
            with open(os.path.join(save_dir, "qa_bm25.pkl"), 'wb') as f:
                pickle.dump(self.qa_bm25, f)
            
            with open(os.path.join(save_dir, "qa_docs.pkl"), 'wb') as f:
                pickle.dump(self.qa_docs, f)
            
            with open(os.path.join(save_dir, "qa_metadata.json"), 'w', encoding='utf-8') as f:
                json.dump(self.qa_metadata, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é…ç½®
        config = {
            "bm25_type": self.bm25_type,
            "k1": self.k1,
            "b": self.b,
            "created_at": datetime.now().isoformat()
        }
        with open(os.path.join(save_dir, "config.json"), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"BM25ç´¢å¼•å·²ä¿å­˜åˆ° {save_dir}")
    
    def load_index(self, save_dir: str = "bm25_index"):
        """åŠ è½½å·²ä¿å­˜çš„BM25ç´¢å¼•"""
        logger.info(f"ä» {save_dir} åŠ è½½BM25ç´¢å¼•...")
        
        # åŠ è½½é…ç½®
        with open(os.path.join(save_dir, "config.json"), 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self.bm25_type = config["bm25_type"]
        self.k1 = config["k1"]
        self.b = config["b"]
        
        # åŠ è½½å•†å“ç´¢å¼•
        try:
            with open(os.path.join(save_dir, "product_bm25.pkl"), 'rb') as f:
                self.product_bm25 = pickle.load(f)
            
            with open(os.path.join(save_dir, "product_docs.pkl"), 'rb') as f:
                self.product_docs = pickle.load(f)
            
            with open(os.path.join(save_dir, "product_metadata.json"), 'r', encoding='utf-8') as f:
                self.product_metadata = json.load(f)
            
            logger.info(f"å•†å“ç´¢å¼•åŠ è½½å®Œæˆ: {len(self.product_docs)} ä¸ªæ–‡æ¡£")
        except FileNotFoundError:
            logger.warning("æœªæ‰¾åˆ°å•†å“ç´¢å¼•æ–‡ä»¶")
        
        # åŠ è½½é—®ç­”ç´¢å¼•
        try:
            with open(os.path.join(save_dir, "qa_bm25.pkl"), 'rb') as f:
                self.qa_bm25 = pickle.load(f)
            
            with open(os.path.join(save_dir, "qa_docs.pkl"), 'rb') as f:
                self.qa_docs = pickle.load(f)
            
            with open(os.path.join(save_dir, "qa_metadata.json"), 'r', encoding='utf-8') as f:
                self.qa_metadata = json.load(f)
            
            logger.info(f"é—®ç­”ç´¢å¼•åŠ è½½å®Œæˆ: {len(self.qa_docs)} ä¸ªæ–‡æ¡£")
        except FileNotFoundError:
            logger.warning("æœªæ‰¾åˆ°é—®ç­”ç´¢å¼•æ–‡ä»¶")
    
    def analyze_query(self, query: str) -> Dict:
        """åˆ†ææŸ¥è¯¢ï¼Œè¿”å›åˆ†è¯ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯"""
        tokens = self.tokenizer.tokenize(query)
        
        analysis = {
            "original_query": query,
            "tokens": tokens,
            "token_count": len(tokens),
            "unique_tokens": len(set(tokens))
        }
        
        return analysis
    
    def get_index_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "bm25_type": self.bm25_type,
            "parameters": {"k1": self.k1, "b": self.b},
            "product_index": {
                "document_count": len(self.product_docs) if self.product_docs else 0,
                "avg_doc_length": np.mean([len(doc) for doc in self.product_docs]) if self.product_docs else 0
            },
            "qa_index": {
                "document_count": len(self.qa_docs) if self.qa_docs else 0,
                "avg_doc_length": np.mean([len(doc) for doc in self.qa_docs]) if self.qa_docs else 0
            }
        }
        
        return stats

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•BM25æœç´¢å¼•æ“"""
    print("ğŸ” BM25å…³é”®è¯æœç´¢å¼•æ“æµ‹è¯•")
    print("="*50)
    
    # åˆ›å»ºæœç´¢å¼•æ“
    search_engine = BM25SearchEngine(bm25_type="BM25Okapi")
    
    try:
        # åŠ è½½æ•°æ®å¹¶æ„å»ºç´¢å¼•
        search_engine.load_data()
        
        # ä¿å­˜ç´¢å¼•
        search_engine.save_index()
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = search_engine.get_index_stats()
        print(f"\nğŸ“Š ç´¢å¼•ç»Ÿè®¡:")
        print(f"ç®—æ³•ç±»å‹: {stats['bm25_type']}")
        print(f"å•†å“æ–‡æ¡£æ•°: {stats['product_index']['document_count']}")
        print(f"é—®ç­”æ–‡æ¡£æ•°: {stats['qa_index']['document_count']}")
        
        # æµ‹è¯•æœç´¢
        test_queries = [
            "iPhoneæ‰‹æœº",
            "ä¾¿å®œçš„è¡£æœ",
            "è¿˜åœ¨å—",
            "èƒ½ä¾¿å®œç‚¹å—",
            "åä¸ºæ‰‹æœº",
            "Nikeé‹å­"
        ]
        
        print(f"\nğŸ§ª æµ‹è¯•æœç´¢:")
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            
            # åˆ†ææŸ¥è¯¢
            analysis = search_engine.analyze_query(query)
            print(f"åˆ†è¯ç»“æœ: {analysis['tokens']}")
            
            # æœç´¢å•†å“
            products = search_engine.search_products(query, top_k=3)
            print(f"å•†å“ç»“æœ ({len(products)} ä¸ª):")
            for i, product in enumerate(products, 1):
                print(f"  {i}. [{product['score']:.3f}] {product['title']}")
            
            # æœç´¢é—®ç­”
            qa_pairs = search_engine.search_qa_pairs(query, top_k=2)
            print(f"é—®ç­”ç»“æœ ({len(qa_pairs)} ä¸ª):")
            for i, qa in enumerate(qa_pairs, 1):
                print(f"  {i}. [{qa['score']:.3f}] {qa['question']}")
            
            print("-" * 40)
        
        print(f"\nâœ… BM25æœç´¢å¼•æ“æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()