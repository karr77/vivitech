import json
import numpy as np
import pandas as pd
import re
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
import pickle
import os
from tqdm import tqdm
import logging
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class XianyuEmbeddingGenerator:
    """é—²é±¼æ•°æ®Embeddingç”Ÿæˆå™¨"""
    
    def __init__(self, model_name: str = "shibing624/text2vec-base-chinese"):
        """
        åˆå§‹åŒ–Embeddingç”Ÿæˆå™¨
        
        Args:
            model_name: ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åç§°
                - "shibing624/text2vec-base-chinese": è½»é‡çº§ï¼Œé€Ÿåº¦å¿«
                - "moka-ai/m3e-base": æ•ˆæœæ›´å¥½ï¼Œç¨å¤§ä¸€äº›
                - "BAAI/bge-large-zh": æœ€æ–°SOTAï¼Œä½†æ¨¡å‹è¾ƒå¤§
        """
        self.model_name = model_name
        self.model = None
        self.vector_dim = None
        
        # æ–‡æœ¬æ¸…æ´—æ­£åˆ™æ¨¡å¼
        self.clean_patterns = [
            (r'[^\w\s\u4e00-\u9fff]', ''),  # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼
            (r'\s+', ' '),  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
            (r'^\s+|\s+$', ''),  # å»é™¤é¦–å°¾ç©ºæ ¼
        ]
        
        # åœç”¨è¯ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        self.stop_words = set([
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 
            'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 
            'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
        ])

    def load_model(self) -> None:
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            
            # è·å–å‘é‡ç»´åº¦
            test_text = "æµ‹è¯•æ–‡æœ¬"
            test_embedding = self.model.encode(test_text)
            self.vector_dim = len(test_embedding)
            
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {self.vector_dim}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return ""
        
        # åº”ç”¨æ¸…æ´—è§„åˆ™
        cleaned_text = text.strip()
        for pattern, replacement in self.clean_patterns:
            cleaned_text = re.sub(pattern, replacement, cleaned_text)
        
        # å»é™¤åœç”¨è¯ï¼ˆå¯é€‰ï¼Œå¯¹äºembeddingå¯èƒ½ä¸æ˜¯å¿…é¡»çš„ï¼‰
        # words = cleaned_text.split()
        # cleaned_text = ' '.join([w for w in words if w not in self.stop_words])
        
        return cleaned_text

    def extract_brand_and_type(self, title: str) -> Tuple[str, str]:
        """
        ä»æ ‡é¢˜ä¸­æå–å“ç‰Œå’Œç±»å‹
        
        Args:
            title: å•†å“æ ‡é¢˜
            
        Returns:
            (å“ç‰Œ, ç±»å‹)
        """
        words = title.split()
        brand = words[0] if len(words) > 0 else ""
        product_type = words[1] if len(words) > 1 else ""
        
        return brand, product_type

    def prepare_product_text(self, product: Dict) -> str:
        """
        ä¸ºå•†å“æ•°æ®å‡†å¤‡embeddingæ–‡æœ¬ï¼Œä¼˜åŒ–åŒ¹é…ç­–ç•¥
        
        Args:
            product: å•†å“æ•°æ®å­—å…¸
            
        Returns:
            æ‹¼æ¥åçš„æ–‡æœ¬
        """
        components = []
        
        # æå–å“ç‰Œå’Œç±»å‹ï¼ˆæ ¸å¿ƒåŒ¹é…ä¿¡æ¯ï¼‰
        brand = ""
        product_type = ""
        
        if product.get('title'):
            title = product['title']
            components.append(title)  # åŸå§‹æ ‡é¢˜
            components.append(title)  # é‡å¤ä¸€æ¬¡æé«˜æƒé‡
            
            # å°è¯•æå–å“ç‰Œå’Œç±»å‹
            words = title.split()
            if len(words) > 0:
                brand = words[0]
            if len(words) > 1:
                product_type = words[1]
            
            # å¼ºåŒ–å“ç‰Œå’Œç±»å‹çš„æƒé‡
            if brand and product_type:
                components.append(f"{brand} {product_type}")  # å“ç‰Œ+ç±»å‹
                components.append(f"{brand} {product_type}")  # å†æ¬¡é‡å¤å¢åŠ æƒé‡
        
        # åˆ†ç±»ä¿¡æ¯ï¼ˆæ¬¡è¦åŒ¹é…ä¿¡æ¯ï¼‰
        if product.get('category'):
            components.append(product['category'])
        if product.get('subcategory'):
            subcategory = product['subcategory']
            components.append(subcategory)
            # å¦‚æœå•†å“ç±»å‹ä¸ºç©ºï¼Œä½¿ç”¨subcategory
            if not product_type:
                product_type = subcategory
                # å“ç‰Œ+å­ç±»åˆ«é‡å¤ä»¥å¢å¼ºæƒé‡
                if brand:
                    components.append(f"{brand} {product_type}")
            
        # æè¿°ï¼ˆè¡¥å……ä¿¡æ¯ï¼Œæƒé‡è¾ƒä½ï¼‰
        if product.get('description'):
            description = product['description']
            # å°†æè¿°ä¿¡æ¯æˆªæ–­ï¼Œé¿å…ç¨€é‡Šå…³é”®ç‰¹å¾
            if len(description) > 50:
                description = description[:50]
            components.append(description)
            
        # æˆè‰²ï¼ˆè¡¥å……ä¿¡æ¯ï¼‰
        if product.get('condition'):
            components.append(product['condition'])
        
        # æ‹¼æ¥æ–‡æœ¬
        combined_text = ' '.join(components)
        return self.clean_text(combined_text)

    def prepare_qa_text(self, qa_pair: Dict) -> Tuple[str, str]:
        """
        ä¸ºé—®ç­”å¯¹å‡†å¤‡embeddingæ–‡æœ¬
        
        Args:
            qa_pair: é—®ç­”å¯¹æ•°æ®
            
        Returns:
            (question_text, answer_text)
        """
        question = qa_pair.get('question', '')
        answer = qa_pair.get('answer', '')
        
        # ä¸ºé—®é¢˜æ·»åŠ é—®é¢˜ç±»å‹ä¿¡æ¯å¹¶å¢å¼ºæƒé‡
        if qa_pair.get('question_type'):
            question_type = qa_pair['question_type']
            # é—®é¢˜ç±»å‹é‡å¤ä»¥å¢å¼ºæƒé‡
            question = f"{question_type} {question_type} {question}"
        
        return self.clean_text(question), self.clean_text(answer)

    def generate_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç”Ÿæˆembeddings
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            embeddingsæ•°ç»„
        """
        if not self.model:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        embeddings = []
        
        logger.info(f"å¼€å§‹ç”Ÿæˆ{len(texts)}ä¸ªæ–‡æœ¬çš„embeddings...")
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(texts), batch_size), desc="ç”Ÿæˆembeddings"):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(batch_texts, 
                                               convert_to_numpy=True,
                                               show_progress_bar=False)
            embeddings.append(batch_embeddings)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        all_embeddings = np.vstack(embeddings)
        logger.info(f"embeddingsç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {all_embeddings.shape}")
        
        return all_embeddings

    def process_products(self, products: List[Dict]) -> Tuple[List[str], np.ndarray, List[Dict]]:
        """
        å¤„ç†å•†å“æ•°æ®
        
        Args:
            products: å•†å“åˆ—è¡¨
            
        Returns:
            (texts, embeddings, processed_products)
        """
        logger.info(f"å¼€å§‹å¤„ç†{len(products)}ä¸ªå•†å“...")
        
        texts = []
        processed_products = []
        
        for product in products:
            # å‡†å¤‡æ–‡æœ¬
            text = self.prepare_product_text(product)
            if text:  # åªå¤„ç†éç©ºæ–‡æœ¬
                texts.append(text)
                
                # æ·»åŠ å¤„ç†åçš„å•†å“ä¿¡æ¯
                processed_product = product.copy()
                processed_product['embedding_text'] = text
                processed_products.append(processed_product)
        
        # ç”Ÿæˆembeddings
        embeddings = self.generate_embeddings_batch(texts)
        
        logger.info(f"å•†å“å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆå•†å“æ•°: {len(processed_products)}")
        return texts, embeddings, processed_products

    def process_qa_pairs(self, qa_pairs: List[Dict]) -> Tuple[List[str], List[str], np.ndarray, np.ndarray, List[Dict]]:
        """
        å¤„ç†é—®ç­”å¯¹æ•°æ®
        
        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            
        Returns:
            (question_texts, answer_texts, question_embeddings, answer_embeddings, processed_qa_pairs)
        """
        logger.info(f"å¼€å§‹å¤„ç†{len(qa_pairs)}ä¸ªé—®ç­”å¯¹...")
        
        question_texts = []
        answer_texts = []
        processed_qa_pairs = []
        
        for qa_pair in qa_pairs:
            question_text, answer_text = self.prepare_qa_text(qa_pair)
            
            if question_text and answer_text:  # åªå¤„ç†éç©ºçš„é—®ç­”å¯¹
                question_texts.append(question_text)
                answer_texts.append(answer_text)
                
                # æ·»åŠ å¤„ç†åçš„é—®ç­”å¯¹ä¿¡æ¯
                processed_qa = qa_pair.copy()
                processed_qa['question_embedding_text'] = question_text
                processed_qa['answer_embedding_text'] = answer_text
                processed_qa_pairs.append(processed_qa)
        
        # åˆ†åˆ«ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆçš„embeddings
        question_embeddings = self.generate_embeddings_batch(question_texts)
        answer_embeddings = self.generate_embeddings_batch(answer_texts)
        
        logger.info(f"é—®ç­”å¯¹å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆé—®ç­”å¯¹æ•°: {len(processed_qa_pairs)}")
        return question_texts, answer_texts, question_embeddings, answer_embeddings, processed_qa_pairs

    def save_embeddings(self, save_dir: str = "embeddings_data"):
        """
        ä¿å­˜æ‰€æœ‰embeddingæ•°æ®
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜äº§å“embeddings
        if hasattr(self, 'product_embeddings'):
            np.save(os.path.join(save_dir, "product_embeddings.npy"), self.product_embeddings)
            logger.info("å•†å“embeddingså·²ä¿å­˜")
        
        # ä¿å­˜é—®ç­”embeddings
        if hasattr(self, 'question_embeddings'):
            np.save(os.path.join(save_dir, "question_embeddings.npy"), self.question_embeddings)
            np.save(os.path.join(save_dir, "answer_embeddings.npy"), self.answer_embeddings)
            logger.info("é—®ç­”embeddingså·²ä¿å­˜")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        if hasattr(self, 'processed_products'):
            with open(os.path.join(save_dir, "processed_products.json"), 'w', encoding='utf-8') as f:
                json.dump(self.processed_products, f, ensure_ascii=False, indent=2)
            logger.info("å¤„ç†åçš„å•†å“æ•°æ®å·²ä¿å­˜")
        
        if hasattr(self, 'processed_qa_pairs'):
            with open(os.path.join(save_dir, "processed_qa_pairs.json"), 'w', encoding='utf-8') as f:
                json.dump(self.processed_qa_pairs, f, ensure_ascii=False, indent=2)
            logger.info("å¤„ç†åçš„é—®ç­”æ•°æ®å·²ä¿å­˜")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "model_name": self.model_name,
            "vector_dimension": self.vector_dim,
            "product_count": len(getattr(self, 'processed_products', [])),
            "qa_pair_count": len(getattr(self, 'processed_qa_pairs', [])),
            "generated_at": datetime.now().isoformat()
        }
        
        with open(os.path.join(save_dir, "metadata.json"), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ° {save_dir} ç›®å½•")

    def load_mock_data(self, file_path: str) -> Dict:
        """
        åŠ è½½mockæ•°æ®
        
        Args:
            file_path: mockæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ•°æ®å­—å…¸
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"å·²åŠ è½½mockæ•°æ®: {len(data['products'])}ä¸ªå•†å“, {len(data['qa_pairs'])}ä¸ªé—®ç­”å¯¹")
        return data

    def process_all_data(self, mock_data_path: str, save_dir: str = "embeddings_data"):
        """
        å¤„ç†æ‰€æœ‰æ•°æ®çš„ä¸»å‡½æ•°
        
        Args:
            mock_data_path: mockæ•°æ®æ–‡ä»¶è·¯å¾„
            save_dir: ä¿å­˜ç›®å½•
        """
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # åŠ è½½æ•°æ®
        data = self.load_mock_data(mock_data_path)
        
        # å¤„ç†å•†å“æ•°æ®
        product_texts, self.product_embeddings, self.processed_products = self.process_products(data['products'])
        
        # å¤„ç†é—®ç­”æ•°æ®
        question_texts, answer_texts, self.question_embeddings, self.answer_embeddings, self.processed_qa_pairs = self.process_qa_pairs(data['qa_pairs'])
        
        # ä¿å­˜æ‰€æœ‰æ•°æ®
        self.save_embeddings(save_dir)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.print_statistics()

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“Š Embeddingç”Ÿæˆç»Ÿè®¡")
        print("="*50)
        print(f"ğŸ·ï¸  ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“ å‘é‡ç»´åº¦: {self.vector_dim}")
        print(f"ğŸ›ï¸  å•†å“æ•°é‡: {len(self.processed_products)}")
        print(f"â“ é—®ç­”å¯¹æ•°é‡: {len(self.processed_qa_pairs)}")
        print(f"ğŸ“¦ å•†å“embeddingså½¢çŠ¶: {self.product_embeddings.shape}")
        print(f"â“ é—®é¢˜embeddingså½¢çŠ¶: {self.question_embeddings.shape}")
        print(f"ğŸ’¬ ç­”æ¡ˆembeddingså½¢çŠ¶: {self.answer_embeddings.shape}")
        
        # æ˜¾ç¤ºä¸€äº›æ ·ä¾‹
        print("\nğŸ“ æ ·ä¾‹æ•°æ®:")
        if self.processed_products:
            print(f"å•†å“æ ·ä¾‹æ–‡æœ¬: {self.processed_products[0]['embedding_text'][:100]}...")
        if self.processed_qa_pairs:
            print(f"é—®é¢˜æ ·ä¾‹æ–‡æœ¬: {self.processed_qa_pairs[0]['question_embedding_text']}")
            print(f"ç­”æ¡ˆæ ·ä¾‹æ–‡æœ¬: {self.processed_qa_pairs[0]['answer_embedding_text']}")

    def test_similarity(self, query: str, top_k: int = 5):
        """
        æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›top-kç»“æœ
        """
        if not self.model:
            logger.error("æ¨¡å‹æœªåŠ è½½")
            return
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_text = self.clean_text(query)
        query_embedding = self.model.encode([query_text])
        
        # è®¡ç®—ä¸å•†å“çš„ç›¸ä¼¼åº¦ - ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        if hasattr(self, 'product_embeddings'):
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
            product_similarities = cosine_similarity(
                self.product_embeddings, query_embedding
            ).flatten()
            top_product_indices = np.argsort(product_similarities)[::-1][:top_k]
            
            print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
            print("ğŸ“¦ æœ€ç›¸ä¼¼çš„å•†å“:")
            for i, idx in enumerate(top_product_indices, 1):
                similarity = product_similarities[idx]
                product = self.processed_products[idx]
                print(f"{i}. [{similarity:.3f}] {product['title']}")
        
        # è®¡ç®—ä¸é—®é¢˜çš„ç›¸ä¼¼åº¦ - ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        if hasattr(self, 'question_embeddings'):
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
            question_similarities = cosine_similarity(
                self.question_embeddings, query_embedding
            ).flatten()
            top_question_indices = np.argsort(question_similarities)[::-1][:top_k]
            
            print("\nâ“ æœ€ç›¸ä¼¼çš„é—®é¢˜:")
            for i, idx in enumerate(top_question_indices, 1):
                similarity = question_similarities[idx]
                qa_pair = self.processed_qa_pairs[idx]
                print(f"{i}. [{similarity:.3f}] {qa_pair['question']}")
                print(f"   ç­”æ¡ˆ: {qa_pair['answer']}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    MOCK_DATA_PATH = "tests/xianyu_mock_data.json"
    SAVE_DIR = "embeddings_data"
    MODEL_NAME = "shibing624/text2vec-base-chinese"  # å¯ä»¥æ”¹ä¸ºå…¶ä»–æ¨¡å‹
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = XianyuEmbeddingGenerator(model_name=MODEL_NAME)
    
    try:
        # å¤„ç†æ‰€æœ‰æ•°æ®
        generator.process_all_data(MOCK_DATA_PATH, SAVE_DIR)
        
        # æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
        print("\nğŸ§ª æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢:")
        test_queries = [
            "iPhoneæ‰‹æœº",
            "ä¾¿å®œçš„è¡£æœ",
            "è¿˜åœ¨å—",
            "èƒ½ä¾¿å®œç‚¹å—"
        ]
        
        for query in test_queries:
            generator.test_similarity(query, top_k=3)
            print("-" * 50)
            
        print("\nâœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()